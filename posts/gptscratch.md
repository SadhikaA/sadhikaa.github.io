---
title: 'Building GPT from Scratch'
date: '2024-01-06'
---

## Introduction 

> **Attention is All You Need**
> 

- GPT = generatively pre-trained transformer
- transformer = neural net that does the computing
- let's make a character-level model trained on tiny Shakespeare data

## **todo**: add picture of the model