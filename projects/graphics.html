<html>

<head>
    <title>Sadhika Akula</title>
    <meta charset="UTF-8">
    <link rel="stylesheet" type="text/css" href="../styles.css">
    <meta name="description" content="personal website of Sadhika Akula" />
    <meta name="environment" content="url=f1_racetrack.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="icon" href="/icons/" sizes="any" />
    <link rel="favicon" href="/icons/favicon.png" />
    <link rel="apple-touch-icon" href="/favicon.png" />
</head>

<body class="container">
    <h2>ü´ñ CS 184 Graphics Assignments</h2>
    <div class="grid">
        <p class="contentTag">CGL (C Graphics Lib)</p>
    </div>
    <hr>
    </hr>
    <ul>
        <li><a href="#1" class="tocLink">Assignment 1: Rasterization</a></li>
        <ul style="padding-left: 20px;">
            <li><a href="#1a" class="tocLink">Drawing Single-Color Triangles</a></li>
            <li><a href="#1b" class="tocLink">Antialiasing by Supersampling</a></li>
            <li><a href="#1c" class="tocLink">Transforms</a></li>
            <li><a href="#1d" class="tocLink">Barycentric Coordinates</a></li>
            <li><a href="#1e" class="tocLink">"Pixel Sampling" for Texture Mapping</a></li>
            <li><a href="#1f" class="tocLink">"Level Sampling" with mipmaps for Texture Mapping</a></li>
        </ul>
        <li><a href="#2" class="tocLink">Assignment 2: Mesh Geometry</a></li>
        <ul style="padding-left: 20px">
            <li><a href="#2a" class="tocLink">Bezier Curves with 1D de Casteljau Subdivision</a></li>
            <li><a href="#2b" class="tocLink">Bezier Curves with Seperable 1D de Casteljau</a></li>
            <li><a href="#2c" class="tocLink">Area-Weighted Vertex Normals</a></li>
            <li><a href="#2d" class="tocLink">Edge Flip</a></li>
            <li><a href="#2e" class="tocLink">Edge Split</a></li>
            <li><a href="#2f" class="tocLink">Loop Subdivision for Mesh Upsampling</a></li>
        </ul>
        <li><a href="#3" class="tocLink">Assignment 3: Ray Tracing</a></li>
        <ul style="padding-left: 20px">
            <li><a href="#3a" class="tocLink">Ray Generation and Scene Intersection</a></li>
            <!-- <li><a href="#3b" class="tocLink">Bounding Volume Hierarchy</a></li>
            <li><a href="#3c" class="tocLink">Direct Illumination</a></li>
            <li><a href="#3d" class="tocLink">Global Illumination</a></li>
            <li><a href="#3e" class="tocLink">Adaptive Sampling</a></li> -->
        </ul>
        <!-- <li><a href="#4" class="tocLink">Assignment 4: Animation</a></li>
        <ul style="padding-left: 20px">
            <li><a href="#4a" class="tocLink">Masses and Springs</a></li>
            <li><a href="#4b" class="tocLink">Simulation via Numerical Integratio</a></li>
            <li><a href="#4c" class="tocLink">Handling Collisions</a></li>
            <li><a href="#4d" class="tocLink">Handling Self-Collisions</a></li>
            <li><a href="#4e" class="tocLink">Shaders</a></li>
        </ul> -->
    </ul>
    <hr>
    </hr>
    <h3 id="1">Rasterization</h3>
    <p>In this homework, the goal was to implement a simple rasterizer with features such as drawing triangles,
        supersampling, hierarchal transforms, and texture mapping with antialiasing, resulting in a functional vector
        graphics renderer. I learnt quite a bit about each of the sampling algorithms and texture mapping algorithms
        implemented throughout the project. I also learnt more about the sampling pipeline and how to represent the
        abstractions of sample buffer, pixels, texels, and coordinate systems through the data structures used in this
        project.</p>
    <h4 id="1a">Drawing Single-Color Triangles</h4>
    <p>In order to rasterize triangles to the frame buffer, I check if the center of each pixel is within the triangle.
        The <code>rasterize_triangle</code> function takes in coordinates for each of the three corners of the triangle.
        My implementation first determines what the smallest bounding box (calculated by determining the lowest and
        highest x and y values that the triangle covered) around the triangle. Then, it iterates through each pixel in
        the bounding box and checks if the center of the pixel is within the triangle. We check if the center of the
        pixel <code>(x + 0.5, y + 0.5)</code> is in the triangle by checking to see that it is within the three edges of
        the triangle. If it is, the pixel is filled in with the color of the triangle. This handles coordinates received
        in the clockwise direction, but to include coordinates in the counter-clockwise direction, I reversed the
        inequality check.</p>
    <p>This algorithm is no worse than traversing each sample individually in the bounding box of the triangle and
        determining if it's in the triangle because we still iterate through all the points in the bounding box. There
        is no optimization or short circuiting that reduces the number of pixels traversed.</p>
    <h4 id="1b">Antialiasing by Supersampling</h4>
    <p>In this task, I implemented supersampling by updating the <code>sample_buffer</code> data structure and modifying
        certain algorithms. Supersampling is useful because it allows us to antialias and reduce the jaggies in our
        images, making pixels seem smoother zoomed out. The modifications I made to the rasterization pipeline was to
        first change the size of the <code>sample_buffer</code> to include the number of samples we wanted to sample,
        <code>width * height * sample_rate</code>. Now, each <code>(x, y)</code> pixel will be represented by a
        sample_rate number of samples that will be averaged in order to downsample to determine the color of the
        original pixel. I added a new <code>fill_sample</code> function which fills in the color of a specific sample
        for an <code>(x, y)</code> pixel and updates its color in the sample_buffer by using a new indexing method:
        <code>sample_buffer[sample_rate * (y * width + x) + s]</code>. I updated the <code>fill_pixel</code> sample to
        stay consistent for points and lines by calling <code>fill_sample</code> for the number of samples in the pixel,
        effectively making sure that all samples have the same color. In <code>rasterize_triangle</code>, I now iterated
        through each sample, rather than pixel to check if we were within bounds, and called <code>fill_sample</code>
        instead of <code>fill_pixel</code>. Finally, in <code>resolve_to_framebuffer</code>, I updated the frame buffer
        by getting all samples that corresponded to an <code>(x, y)</code> pixel and averaging the RGB values of all the
        samples to get the final color, which was then resolved to the framebuffer target.
    </p>
    <div class="image-row">
        <img src="../images/graphics/hw1/test4_1.png" class="proj-image" style="width: 25%">
        <img src="../images/graphics/hw1/test4_4.png" class="proj-image" style="width: 25%">
        <img src="../images/graphics/hw1/test4_9.png" class="proj-image" style="width: 25%">
        <img src="../images/graphics/hw1/test4_16.png" class="proj-image" style="width: 25%">
    </div>
    <p class="caption">Sample rates of 1, 4, 9, and 16</p>
    <h4 id="1c">Transforms</h4>
    <div class="images">
        <img src="../images/graphics/hw1/robot.png" class="proj-image" style="width: 60%">
        <p class="caption">Cubeman stands on their head</p>
    </div>
    <h4 id="1d">Barycentric Coordinates</h4>
    <p>Barycentric coordinates work by determining our color based on the distance each point is from each vertex. As we
        can see with our triangle, each vertex is red, green, or blue. We end up with a gradient triangle as our final
        image because for each point not at the vertices, we calculate a certain weight of red, green, and blue based on
        distance and end up with a weighted sum of RGB colors within the triangle.</p>
    <p>We calculate the barycentric coordinates by solving for the value of each our new color coordinate system
        (<code>alpha</code>, <code>beta</code>, and <code>gamma</code>) based on the current <code>(x, y)</code>
        coordinate. This allows us to assign colors based on the strength of how close each of the points are too the .
        We then calculate the color by interpolating across the colors passed in to the function to get a weighted sum:
        <code>alpha * c0 + beta * c1 + gamma * c2</code>. This is our new value for the color at the point
        <code>(x, y)</code> so we call <code>fill_sample</code> with the new color.
    </p>
    <div class="image-row">
        <img src="../images/graphics/hw1/bary.png" class="proj-image" style="width: 50%">
        <img src="../images/graphics/hw1/bary_triangle.png" class="proj-image" style="width: 50%">
    </div>
    <p class="caption">Barycentric circle and triangle</p>
    <h4 id="1d">"Pixel Sampling" for Texture Mapping</h4>
    <p>With texture mapping, the goal is to interpolate to find the texture coordinate <code>(u, v)</code> for each
        <code>(x, y)</code> coordinate. We can do this using nearest and bilinear sampling methods.
    </p>
    <p>I implemented texture mapping by calculating the <code>uv</code> coordinates based on the barycentric coordinates
        that had been calculated. I initialized the <code>SampleParams</code> struct with the <code>uv</code> values and
        then sampled from the texture based on which sampling method we wanted to use. The <code>sample</code> function
        returns a color which can then be used to fill the sample.</p>
    <p>In the <code>sample_nearest</code> function, I found the nearest pixel to our <code>uv</code> coordinate and then
        returned the color of that pixel on the mipmap. To implement <code>sample_bilinear</code>, I got the nearest 4
        texels to our <code>uv</code> coordinate. After, getting the colors, I linearly interpolated in both directions
        and then did a final interpolation of the two interpolations to get the final color value.</p>
    <div class="image-row">
        <img src="../images/graphics/hw1/nearest_1_side.png" class="proj-image" style="width: 50%">
        <img src="../images/graphics/hw1/nearest_16_side.png" class="proj-image" style="width: 50%">
    </div>
    <p class="caption">Nearest with sample_rate 1 and 16</p>
    <div class="image-row">
        <img src="../images/graphics/hw1/bilinear_1_side.png" class="proj-image" style="width: 50%">
        <img src="../images/graphics/hw1/bilinear_16_side.png" class="proj-image" style="width: 50%">
    </div>
    <p class="caption">Bilinear with sample_rate 1 and 16</p>
    <p>Based on these images, a higher sampling rate helps create a blurring effect that reduces jaggies no matter which
        sampling method is used for texture mapping. Bilinear seems to be better at blending nearby texels as we zoom in
        and creates the illusion of straight lines even without a higher sampling rate. Combining both bilinear sampling
        and high sampling rate allows us to get smooth and clear details even when zoomed in quite a bit. The tradeoffs
        are that bilinear sampling takes more time to render. If we have an image with lots of finer detail, bilinear
        sampling will be able to preserve these details much more than nearest sampling.</p>
    <h4 id="1e">"Level Sampling" with mipmaps for Texture Mapping</h4>
    <p>In level sampling, we add the calculation of samples that represent the downsampled image that we can use to
        improve the resolution and pick better texture samples that correspond with the amount of aliasing occuring.The
        modifications in the calculation include calculating derivatives that quantify the difference between texture
        coordinates that are near each other. In order to make these changes smoother, I used linear interpolation again
        to smooth out the shifts between mipmap levels. The <code>sample</code> function is modified to allow for both
        pixel sampling and level sampling to get trilinear texture filtering.</p>
    <p>Now that the sampling technique can be modified using pixel sampling, level sampling, and
        <code>sample_rate</code>, I've found that the pixel sampling and level sampling take longer to render than
        changes to <code>sample_rate</code>. Antialiasing seems better with level sampling and supersampling. I think
        level sampling and pixel sampling work better when we are aiming to have our textures be smoother as we zoom in.
        Zooming in is definitely the greatest memory usage and speed in terms of having the ability to render everything
        at a specific framerate.
    </p>
    <div class="image-row">
        <img src="../images/graphics/hw1/zero_nearest.png" class="proj-image" style="width: 50%">
        <img src="../images/graphics/hw1/bilinear_16_side.png" class="proj-image" style="width: 50%">
    </div>
    <p class="caption">[L_ZERO and P_NEAREST] and [L_ZERO and P_LINEAR]</p>
    <div class="image-row">
        <img src="../images/graphics/hw1/nearest_nearest.png" class="proj-image" style="width: 50%">
        <img src="../images/graphics/hw1/nearest_linear.png" class="proj-image" style="width: 50%">
    </div>
    <p class="caption">[L_NEAREST and P_NEAREST] and [L_NEAREST and P_LINEAR]</p>
    <h3 id="2">Mesh Geometry</h3>
    <p>In this homework, the goal was to implement a geometric modeling techniques. I was able to implement the topics
        we had learnt in class including Bezier curves and practice half edge mesh representations throughout this
        homework. The most interesting part (and challenging) was understanding and correctly using the half edge mesh
        representation. I found that drawing images of before and after and writing out the pointers before coding to be
        helpful for this. I've had previous experience doing modeling in Maya so it was really cool to see the math and
        look at what happens to each triangle when you add subdivisions or box model.</p>
    <h4 id="2a">Bezier Curves with 1D de Casteljau Subdivision</h4>
    <p>De Casteljau's algorithm works by using recursive linear interpolation to evaluate points on Bezier curves.
        Linear interpolation works by computing the control points at the previous level, based on <code>t</code>. Given
        an original <code>n</code> points, we evaluate <code>n - 1</code> points to get a new set of points. We
        recursively repeat this until we get a final point.</p>
    <div class="image-row">
        <img src="../images/graphics/hw2/e0.png" class="proj-image" style="width: 33%">
        <img src="../images/graphics/hw2/e1.png" class="proj-image" style="width: 33%">
        <img src="../images/graphics/hw2/e2.png" class="proj-image" style="width: 33%">
    </div>
    <p class="caption">step 1, step 2, step 3</p>
    <div class="image-row">
        <img src="../images/graphics/hw2/e3.png" class="proj-image" style="width: 33%">
        <img src="../images/graphics/hw2/e4.png" class="proj-image" style="width: 33%">
        <img src="../images/graphics/hw2/e5.png" class="proj-image" style="width: 33%">
    </div>
    <p class="caption">step 3, step 4, step 5</p>
    <div class="image-row">
        <img src="../images/graphics/hw2/c1.png" class="proj-image" style="width: 50%">
        <img src="../images/graphics/hw2/c2.png" class="proj-image" style="width: 50%">
    </div>
    <p class="caption">modifying parameter <code>t</code></p>
    <h4 id="2b">Bezier Curves with Seperable 1D de Casteljau</h4>
    <p>First, I extend de Casteljau to Bezier surfaces by first calculating the <code>evaluateStep</code> on 3D points
        instead of 2D points. I then implemented the <code>evaluate1D</code> function by recursively calling
        <code>evaluateStep</code> for all n points and returning the final point. Finally, the <code>evaluate</code>
        function first evaluates <code>n</code> vectors using <code>u</code> as our parameter for <code>t</code>. Then,
        I evaluate these vectors in another axis using <code>v</code> as our parameter, which returns our final point.
        Ultimately, we extend to one more dimension to compute Bezier surfaces.</p>
    <div class="image-row">
        <img src="../images/graphics/hw2/teapot.png" class="proj-image" style="width: 50%">
        <img src="../images/graphics/hw2/teapot_gray.png" class="proj-image" style="width: 50%">
    </div>
    <p class="caption">ü´ñ teapot!</p>
    <h4 id="2c">Area-Weighted Vertex Normals</h4>
    <p>In order have cleaner Phong shading, we calculate area-weighted vertex normals for each face. In order to do this, I first found all vertices for a specific face. To find the normal vector to this face, I took the cross product of two perpendicular vectors and made sure that the normal was pointing outwards. Finally, I summed all the normals across all of the faces and returned the normalized output. Since I'm not modifying any halfedges, I also made sure to use <code>HalfEdgeCIter.</code></p>
    <div class="image-row">
        <img src="../images/graphics/hw2/flat.png" class="proj-image" style="width: 50%">
        <img src="../images/graphics/hw2/phong.png" class="proj-image" style="width: 50%">
    </div>
    <p class="caption">flat shading / phong shading using normals</p>
    <h4 id="2d">Edge Flip</h4>
    <div class="image-row">
        <img src="../images/graphics/hw2/edge_flip.png" class="proj-image" style="width: 100%">
    </div>
    <p>In order to implement the edge flip algorithm, I first drew out this picture above and then wrote out all the pointers that changed throughout the flip and updated them accordingly in the code. Since edge flip doesn't require adding elements or deleting them, I found the implementation pretty straightforward and didn't face any bugs. I realized after completing this that a few operations and assignments could be simplified and accordinly updated that in my edge split implementation.</p>
    <div class="image-row">
        <img src="../images/graphics/hw2/edgeflip1.png" class="proj-image" style="width: 50%">
        <img src="../images/graphics/hw2/edgeflip2.png" class="proj-image" style="width: 50%">
    </div>
    <h4 id="2e">Edge Split</h4>
    <p>To implement, edge split I followed similar steps as edge flip but also added in a new vertex, two additional faces, and new halfedges and edges. I found the <code>setNeighbors</code>function to be quite useful here in order to update all of the pointers of one halfedge at once. Initially, I found no issues with my edge split function but when completing upsampling (below) I realized that a few halfedges were being modified incorrectly which allowed me to fix my implementation. I also added support for labeling the new vertices and edges with <code>isNew = true</code> in the edge split function so that when I edge split in <code>unsample</code> method only the new edges will be flipped.</p>
    <div class="image-row">
        <img src="../images/graphics/hw2/split.png" class="proj-image" style="width: 50%">
        <img src="../images/graphics/hw2/splitflip.png" class="proj-image" style="width: 50%">
    </div>
    <p class="caption">edge split / edge split and flip</p>
    <h4 id="2f">Loop Subdivision for Mesh Upsampling</h4>
    <p>I implemented loop subdivision by following the guidelines in the code. I started off by calculating and saving the new positions of the vertices to be updated in the future, using the equation <code>(1 - n * u) * v-{'>'}position + u * neighborPositionSum</code>. I then computed the vertex position of a current edge using the values of the vertices sharing an edge and the equation <code>(3.0 / 8.0) * (A-{'>'}position + B-{'>'}position) + (1.0 / 8.0) * (C-{'>'}position + D-{'>'}position)</code>. I also saved a list of <code>originalEdges</code> during this process so that I could split only the mesh edges that were from the original mesh in order to not run in an infinite loop. Here, I updated my <code>edgeSplit</code> function to mark new edges as new so that I wouldn't overflip edges. I then flipped edges based on whether it was connecting an old and new vertex and if it was a new edge. Finally, I updated the position of all the vertices.</p>
    <p>As we can see below, the mesh looks nothing like a cube as we increase subdivisions. We achieve a more spherical shape over time but not truly spherical as it maintains part of its squareness.</p>
    <div class="image-row">
        <img src="../images/graphics/hw2/lvl0.png" class="proj-image" style="width: 24%">
        <img src="../images/graphics/hw2/lvl1.png" class="proj-image" style="width: 24%">
        <img src="../images/graphics/hw2/lvl2.png" class="proj-image" style="width: 24%">
        <img src="../images/graphics/hw2/lvl3.png" class="proj-image" style="width: 24%">
    </div>
    <p class="caption">level 0 / level 1 / level 2 / level 3</p>
    <div class="image-row">
        <img src="../images/graphics/hw2/lvl4.png" class="proj-image" style="width: 24%">
        <img src="../images/graphics/hw2/lvl5.png" class="proj-image" style="width: 24%">
        <img src="../images/graphics/hw2/lvl6.png" class="proj-image" style="width: 24%">
        <img src="../images/graphics/hw2/lvl7.png" class="proj-image" style="width: 24%">
    </div>
    <p class="caption">level 4 / level 5 / level 6 / level 7</p>
    <p>By pre-splitting edges, we are able to reduce the sharp corners and maintain the shape of the original image. As we can see below, presplitting only one face keeps the flatness preserved while increasing subdivisions. This preprocessing helps because it creates more edges that can be subdivided which means we make even more edges when we flip, essentially adding more detail at each level.</p>
    <div class="image-row">
        <img src="../images/graphics/hw2/presplit.png" class="proj-image" style="width: 18%">
        <img src="../images/graphics/hw2/presplit1.png" class="proj-image" style="width: 18%">
        <img src="../images/graphics/hw2/presplit2.png" class="proj-image" style="width: 18%">
        <img src="../images/graphics/hw2/presplit3.png" class="proj-image" style="width: 18%">
        <img src="../images/graphics/hw2/presplit4.png" class="proj-image" style="width: 18%">
    </div>
    <p class="caption">presplit / level 1 / level 2 / level 3 / yam üç†</p>
    <p>The usefulness of loop subdivision can be seen on the teapot and the cow which achieves a less geometric look and reduces sharp corners using these operations.</p>
    <div class="image-row">
        <img src="../images/graphics/hw2/tea0.png" class="proj-image" style="width: 24%">
        <img src="../images/graphics/hw2/tea1.png" class="proj-image" style="width: 24%">
        <img src="../images/graphics/hw2/cow0.png" class="proj-image" style="width: 24%">
        <img src="../images/graphics/hw2/cow1.png" class="proj-image" style="width: 24%">
    </div>
    <p class="caption">teapot before / teapot after / cow before / cow after</p>
    <p>Coding up loop subdivision took a bit longer than the other parts due to some pointer bugs and keeping track of which halfedges were new, which led to some crumpled meshes during the debugging:</p>
    <div class="image-row">
        <img src="../images/graphics/hw2/crumpled 1.png" class="proj-image" style="width: 15%">
        <img src="../images/graphics/hw2/crumpled 2.png" class="proj-image" style="width: 15%">
        <img src="../images/graphics/hw2/crumpled 3.png" class="proj-image" style="width: 15%">
        <img src="../images/graphics/hw2/crumpled 4.png" class="proj-image" style="width: 15%">
        <img src="../images/graphics/hw2/crumpled 5.png" class="proj-image" style="width: 15%">
        <img src="../images/graphics/hw2/crumpled 6.png" class="proj-image" style="width: 15%">
    </div>
    <h3 id="3">Ray Tracing</h3>
    <p>In this homework, I learnt about how to generate and represent rays and intersect them with triangles and spheres. After this, I worked on implementing bounding volume hierarchy. After this, was direct and global illumination where I was directly able to see and implement different bounce setups. I also enjoyed working on Russian Roulette sampling and adaptive sampling because it made the concepts we learnt much better. I faced a few bugs with the first few tasks in speeding up computation but I found that I was able to resolve them with a bit of debugging. Overall, I thought that this was a really interesting homework because I was able to implement many of the algorithms that we had only learnt about in class.</p>
    <h4 id="3a">Ray Generation and Scene Intersection</h4>
    <p>The ray generation algorithm implemented allow us to transform coordinates of an image from <code>(x, y)</code> to be coordinates for a Ray in the world space. In order to this, I first converted the <code>(x, y)</code> coordinates to be in the domain <code>[-1, 1]</code> instead of <code>[0, 1]</code>. I then accounted for the field of view calculations, taking into account the fact that <code>hFov</code> and <code>vFov</code> were in radians, not degrees. Finally, I used the <code>c2w</code> matrix to calculate the position of the virtual axis-aligned sensor with respect to our new coordinates. I calculated the normalized version of this vector and then returned the <code>Ray</code> that started from the given <code>pos</code> vector to this newly calculated world space vector.</p>
    <p>To generate pixel samples, for each sample, I picked a random sample by calling <code>gridSampler-{'>'}get_sample()</code>. I then call <code>generateRay</code> on the sampled coordinates, making sure to normalize by the width and the height of the image. For each ray generated in the loop, I call <code>est_radiance_global_illumination</code> and sum up the total illumination from all the rays. I then take the average based on the number of samples and return a final call to <code>update_pixel</code> to render this pixel with the color.</p>
    <p>In order to calculate ray-triangle intersection, I followed the M√∂ller-Trumbore intersection algorithm. In order to use this algorithm, I first calculated a set of variables to make the calculations easier, specifically: <code>E1</code>, <code>E2</code>, <code>S</code>, <code>S1</code>, and <code>S2</code>. I then calculated the inverse determinant and multiplied this by the matrix: <span class='math'>\([S2 \cdot E2, S1 \cdot S, S2 \cdot D]^T\)</span>. This gave me the final matrix, <span class='math'>\([t, b1, b2]^T\)</span>. After getting the values for <code>t</code>, <code>b1</code>, and <code>b2</code>, I tested all of them to make sure that they were within <code>(0, 1)</code>. I repeated this same calculation for the <code>intersect</code> function, but also added updates to the <code>isect</code> variable, by calculating a normal vector based on our values for <code>b1</code>, <code>b2</code>, and <code>1 - b1 - b2</code>.</p>
    <p>In order to calculate ray-sphere intersection, I followed the description from the slides and generated a quadratic equation for the intersection of the ray and the sphere. In order to simplify the calculations, I returned <code>false</code> if the determinant was less than zero because we want only positive roots. I then followed the same steps as ray-triangle intersection to assign values to our intersection. I also made sure to update the max clipping frame with <code>t</code>.</p>
    <div class="image-row">
        <img src="../images/graphics/hw3/rayS.png" class="proj-image" style="width: 24%">
        <img src="../images/graphics/hw3/rayD.png" class="proj-image" style="width: 24%">
        <img src="../images/graphics/hw3/rayB.png" class="proj-image" style="width: 24%">
    </div>
    <p class="caption">ray-sphere intersection / dragon normal shading / banana normal shading</p>
</body>

</html>